# 神经网络与 MCTS 结合的围棋 AI 原理（逐步讲解）

本文面向希望理解本项目（围棋 AI）核心算法的人，按“特征 → 网络 → 搜索 → 训练”顺序拆解，并结合博弈论直观解释。

---

## 1. 围棋状态如何喂给神经网络
1. **棋盘张量化**：将 19×19 棋盘编码为 15 通道特征图（空/本方/对方、气数 1-8、最近 3 步 one-hot、常数平面）。  
2. **规则前置**：`Go` 类保证落子、提子、打劫合法，避免把“非法局面”喂给网络。  
3. **博弈论直觉**：特征把“气、劫、历史”显式提供给网络，相当于让网络知道“当前局面可行性与紧迫性”，为后续策略/价值评估打地基。

---

## 2. 策略网络（PolicyNet）：给出“走子先验”
- **结构**：多层卷积 + BatchNorm → 输出 362 维概率（19×19 + pass）。  
- **作用**：为 MCTS 提供“先验分布”，告诉搜索哪些点更可能是好棋。  
- **为何有效**：卷积捕获局部模式（如劫争、打入、定型），BatchNorm 稳定训练；输出的概率本质上是“博弈中理性的先验”。  
- **输出解释**：每个位置的概率 ≈ “在当前局面把子落在此的优先级”。pass 概率用于终局或无利可图时的策略选择。

---

## 3. 快速策略网络（PlayoutNet）：加速模拟的“轻量棋手”
- **结构**：轻量卷积 + 全连接，较 PolicyNet 简化。  
- **作用**：在 MCTS 模拟（roll-out）阶段快速落子，牺牲精度换取速度。  
- **博弈论类比**：像一个“快速弱棋手”用于大量试探，帮助评估局面趋势。

---

## 4. 价值网络（ValueNet）：给出“胜率评价”
- **结构**：卷积 + 全连接 + Sigmoid → 输出 0-1 胜率。  
- **作用**：直接评估当前局面下本方获胜概率。  
- **为何重要**：MCTS 在中后盘节点很深，完全依赖 roll-out 代价高且噪声大；ValueNet 提供低噪声的“静态评价”，减少长链模拟。

---

## 5. 蒙特卡洛树搜索（MCTS）：把先验、模拟与价值结合
1. **选择（Select）**：从根节点开始，按 UCB/PUCT 公式在子节点间平衡“先验概率（PolicyNet）”和“访问次数/平均价值”。  
2. **扩展（Expand）**：对新节点展开可行动作，用策略先验填入初始概率。  
3. **模拟（Simulate）**：使用 PlayoutNet（或纯随机/简化规则）快速走到终局或截止步数，得到模拟结果。  
4. **回溯（Backpropagate）**：沿路径回写访问次数与价值（可混合 ValueNet 评估和模拟结果）。  
5. **落子决策**：选择访问次数最高或价值最高的子节点作为实际落子。  
6. **探索 vs 利用**：UCB/PUCT 让搜索既不陷入局部（利用），也能适度尝试未探索点（探索），在博弈论上相当于寻找“均衡附近的强策略”。

---

## 6. 训练流程：让网络学到“强先验 + 准确价值”
1. **数据来源**：从 SGF 棋谱提取样本  
   - 策略数据：每步棋的特征 → 标签为实际落子分布，保存为 `policyData.pt`。  
   - 价值数据：终局胜负标签，保存为 `valueData.pt`。  
2. **损失函数**：  
   - 策略：交叉熵（让输出分布逼近人类/强棋手落子分布）。  
   - 价值：均方误差 MSE（让胜率预测逼近真实胜负）。  
3. **优化器与调度**：  
   - SGD 或 Adam；StepLR 调度（策略每 15 epoch 衰减，价值每 2 epoch 衰减）。  
4. **训练目标**：  
   - PolicyNet：学到“高质量先验”，在 MCTS 中减少搜索分支。  
   - ValueNet：学到“准确静态评价”，在深层节点减少长模拟。  
5. **推理加速**：  
   - PlayoutNet：独立轻量模型，专为 roll-out 加速。  

---

## 7. 端到端如何协同（以一手落子为例）
1. 前端发送当前棋盘到后端。  
2. 后端使用 PolicyNet 生成先验，MCTS 以此为导向展开；模拟阶段可调用 PlayoutNet。  
3. 节点评价可混合 ValueNet 胜率与模拟结果，回溯更新。  
4. 选访问次数最高的动作作为最终落子；返回给前端，前端渲染并更新棋谱。  
5. 重复上述流程，形成“强先验 + 深搜索”的对弈闭环。

---

## 8. 与博弈论的结合点
- **先验策略 ≈ 合理策略分布**：PolicyNet 输出在博弈论上近似“当前局面的理性策略概率”。  
- **价值评估 ≈ 状态效用**：ValueNet 的胜率是对当前状态效用的估计。  
- **MCTS ≈ 近似最优响应搜索**：通过探索/利用平衡，逼近纳什均衡附近的强策略。  
- **加速与抽样**：PlayoutNet 提供廉价的样本以估计状态价值，降低计算成本。

---

## 9. 相关代码索引
- 特征与规则：`go.py`、`features.py`
- 网络结构：`net.py`（PolicyNet / PlayoutNet / ValueNet）
- 搜索与落子：`genMove.py`（含 MCTS 结合）
- 训练：`train.py`、`train_all.py`
- 服务与接口：`server.py`、`gtp.py`

---

## 10. 运行与验证
- 推理：`genMove.py` / `server.py` 对外暴露 API；前端通过 `/api` 访问。  
- 训练：`train.py` 使用生成的 `policyData.pt` / `valueData.pt`。  
- 可视化与联调：前端（Vite 3000）代理后端（Flask 8000），便于交互式验证。

