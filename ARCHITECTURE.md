# 围棋AI项目代码架构分析

## 项目概述

这是一个仿照 AlphaGo 的围棋 AI 项目，主要实现了：
- **策略网络**（Policy Network）：用于预测下一步最佳落子位置
- **快速策略网络**（Playout Network）：用于 MCTS 模拟中的快速走子
- **价值网络**（Value Network）：评估棋盘位置的胜率
- **蒙特卡洛树搜索**（MCTS）：结合策略网络进行深度搜索

## 整体架构图

```
┌─────────────────────────────────────────────────────────────┐
│                      数据准备层                                │
├─────────────────────────────────────────────────────────────┤
│  filter.py          prepareData.py                           │
│  └─ 筛选棋谱       └─ 从SGF文件提取特征和标签                 │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      核心规则层                                │
├─────────────────────────────────────────────────────────────┤
│  go.py                                                       │
│  └─ Go类: 实现围棋规则（落子、吃子、打劫、气的计算）          │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      特征提取层                                │
├─────────────────────────────────────────────────────────────┤
│  features.py                                                 │
│  └─ 将棋盘状态转换为15通道特征图（棋子、气、历史记录等）      │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      神经网络层                                │
├─────────────────────────────────────────────────────────────┤
│  net.py                                                      │
│  ├─ PolicyNetwork: 策略网络（5层卷积 + BatchNorm）           │
│  ├─ PlayoutNetwork: 快速策略网络（4层卷积，更轻量）          │
│  └─ ValueNetwork: 价值网络（5层卷积 + 全连接）               │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      训练层                                    │
├─────────────────────────────────────────────────────────────┤
│  train.py                                                    │
│  ├─ trainPolicy(): 训练策略网络/快速策略网络                 │
│  └─ trainValue(): 训练价值网络                                │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      推理层                                    │
├─────────────────────────────────────────────────────────────┤
│  genMove.py                                                  │
│  ├─ genMovePolicy(): 直接使用策略网络输出                     │
│  └─ genMoveMCTS(): 使用MCTS搜索                              │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│                      接口层                                    │
├─────────────────────────────────────────────────────────────┤
│  gtp.py                                                      │
│  └─ 实现GTP协议，与围棋客户端（如Sabaki）通信                │
└─────────────────────────────────────────────────────────────┘
```

## 模块详解

### 1. 核心规则模块 (`go.py`)

**职责**：实现围棋的基本规则和逻辑

**核心类**：
- `Go`: 围棋游戏状态类

**主要功能**：
```python
# 核心数据结构
- board: 19×19棋盘，存储棋子（1=黑，-1=白，0=空）
- liberty: 每个位置的气的数量
- previousBoard: 上一回合的棋盘状态（用于打劫检测）
- history: 最近8步的落子历史

# 核心方法
- move(color, x, y): 落子并处理吃子逻辑
- clearColorNear(): 使用DFS检测并移除无气的棋子
- clone(): 深拷贝游戏状态（用于MCTS搜索）
```

**关键算法**：
- 使用深度优先搜索（DFS）检测连通块
- 计算每个连通块的气（liberty）
- 实现打劫（ko）规则：禁止立即回提

### 2. 特征提取模块 (`features.py`)

**职责**：将棋盘状态转换为神经网络可处理的特征

**特征组成**（共15通道）：
```
通道 0-2:  棋子颜色（空白、本方、对方）
通道 3:    常数1平面
通道 4-11: 气数特征（8个通道，分别表示气数为1-8）
通道 12-14:最近3步的落子位置（one-hot编码）
```

**核心函数**：
- `getAllFeatures(go, willPlayColor)`: 提取所有特征
- `colorStoneFeatures()`: 棋子颜色特征
- `libertiesFeatures()`: 气数特征
- `recentOnehotFeatures()`: 历史落子特征

### 3. 神经网络模块 (`net.py`)

**职责**：定义三个深度学习网络结构

#### 3.1 策略网络 (PolicyNetwork)
```
输入: 15通道特征图 (19×19)
结构:
  Conv2d(15→32) + BatchNorm + ReLU
  Conv2d(32→64) + BatchNorm + ReLU
  Conv2d(64→64) + BatchNorm + ReLU
  Conv2d(64→32) + BatchNorm + ReLU
  Conv2d(32→1)
输出: 361个位置的概率 + pass概率 (362维)
```

#### 3.2 快速策略网络 (PlayoutNetwork)
```
输入: 15通道特征图 (19×19)
结构:
  Conv2d(15→16) + ReLU
  Conv2d(16→16) + ReLU
  Conv2d(16→16) + ReLU
  Conv2d(16→1)
  Linear(361→362)
输出: 362维log-softmax概率分布
特点: 更轻量，用于MCTS快速模拟
```

#### 3.3 价值网络 (ValueNetwork)
```
输入: 15通道特征图 (19×19)
结构:
  Conv2d(15→16) + ReLU
  Conv2d(16→16) + ReLU
  Conv2d(16→16) + ReLU
  Conv2d(16→16) + ReLU
  Conv2d(16→2)
  Linear(722→1)
  Sigmoid
输出: 胜率 (0-1之间的标量)
```

### 4. 训练模块 (`train.py`)

**职责**：训练神经网络模型

**训练流程**：
1. 加载预处理的数据（`policyData.pt` 或 `valueData.pt`）
2. 划分训练集和测试集（8:2）
3. 配置优化器和学习率调度器
4. 批量训练并评估

**策略网络训练**：
- 损失函数：交叉熵损失（CrossEntropyLoss）
- 优化器：SGD（momentum=0.9, lr=0.01）
- 学习率调度：StepLR（每15个epoch衰减10倍）
- 批量大小：100

**价值网络训练**：
- 损失函数：均方误差（MSELoss）
- 优化器：Adam（lr=0.001）
- 学习率调度：StepLR（每2个epoch衰减一半）

### 5. 数据准备模块

#### 5.1 数据过滤 (`filter.py`)
- 从`games/`目录查找所有SGF文件
- 筛选条件：
  - 没有让子（`HA[`标记）
  - 日期在2000年后（`DT[20`标记）
- 输出：`games/allValid.txt`

#### 5.2 数据预处理 (`prepareData.py`)
- **策略网络数据**：
  - 从每个棋谱的每一步提取特征
  - 标签为实际落子的位置（361个位置 + pass）
- **价值网络数据**：
  - 从每个棋谱的最终状态提取特征
  - 标签为最终胜负结果

### 6. 走子生成模块 (`genMove.py`)

**职责**：根据当前棋盘状态生成下一步落子

#### 6.1 策略网络直接输出 (`genMovePolicy`)
```python
流程:
1. 提取当前棋盘特征
2. 策略网络预测所有位置的概率
3. 按概率从高到低尝试落子
4. 选择第一个合法位置
```

#### 6.2 MCTS搜索 (`genMoveMCTS`)
```python
流程:
1. 创建MCTS根节点
2. 进行200次MCTS迭代：
   a. 选择(Selection): 使用UCB公式选择节点
   b. 扩展(Expansion): 使用策略网络扩展前2个候选
   c. 模拟(Simulation): 使用快速策略网络模拟5步
   d. 回传(Backpropagation): 使用价值网络评估并回传
3. 选择访问次数最多的子节点
```

**MCTS节点结构**：
- `N`: 访问次数
- `Q`: 累计价值
- `UCB值`: Q/N + sqrt(log(parent.N) / N)

### 7. GTP协议接口 (`gtp.py`)

**职责**：实现Go Text Protocol，与围棋客户端通信

**支持的GTP命令**：
- `boardsize 19`: 设置棋盘大小
- `clear_board`: 清空棋盘
- `play <color> <position>`: 落子
- `genmove <color>`: 生成下一步走子
- `showboard`: 显示棋盘
- `name/version`: 返回程序信息

**工作流程**：
```
1. 接收GTP命令
2. 解析命令并更新Go对象
3. 调用genMovePolicy()或genMoveMCTS()
4. 返回格式化的结果
```

## 数据流

### 训练阶段
```
SGF棋谱文件
    ↓
filter.py (筛选)
    ↓
prepareData.py (特征提取)
    ↓
policyData.pt / valueData.pt
    ↓
train.py (训练)
    ↓
policyNet.pt / playoutNet.pt / valueNet.pt
```

### 推理阶段
```
当前棋盘状态 (Go对象)
    ↓
features.py (提取特征)
    ↓
net.py (神经网络推理)
    ↓
genMove.py (生成走子)
    ↓
gtp.py (返回结果)
```

## 技术特点

### 优点
1. **模块化设计**：各模块职责清晰，易于维护
2. **完整的围棋规则**：实现了吃子、打劫等复杂规则
3. **多种走子策略**：支持纯策略网络和MCTS搜索
4. **标准协议接口**：GTP协议便于与客户端集成

### 限制和改进方向
1. **网络结构较简单**：未使用残差网络，层数较少
2. **数据量有限**：仅使用约2000局棋谱训练
3. **价值网络效果不佳**：MCTS中使用棋子数而非价值网络
4. **无强化学习**：仅使用监督学习，未进行自我对弈

## 文件依赖关系

```
gtp.py
  ├─ genMove.py
  │   ├─ net.py
  │   ├─ go.py
  │   └─ features.py
  └─ go.py

train.py
  ├─ net.py
  ├─ prepareData.py
  │   ├─ go.py
  │   └─ features.py
  └─ go.py

prepareData.py
  ├─ go.py
  └─ features.py

filter.py (独立模块)
```

## 关键配置参数

- **棋盘大小**: 19×19
- **特征通道数**: 15
- **MCTS迭代次数**: 200
- **快速模拟步数**: 5
- **历史记录长度**: 8步（Go类）/ 3步（特征提取）
- **批量大小**: 100
- **训练数据量**: 约2000局棋谱

