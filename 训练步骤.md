# 训练步骤说明

## 当前状态

✅ 已完成：
- 依赖已安装 (torch, numpy, sgfmill)
- games目录存在，包含42,604个有效棋谱
- policyData.pt 已生成（约2.3GB）
- policyNet.pt 已训练（328KB）
- playoutNet.pt 已训练（556KB）

⏳ 待完成：
- valueData.pt 需要准备（价值网络训练数据）
- valueNet.pt 需要训练（价值网络模型）

## 按README步骤执行

### 步骤1: 准备价值网络数据

价值网络需要从棋谱的最终状态提取特征和胜负结果。运行以下命令准备数据：

```bash
python -c "from prepareData import prepareValueData; prepareValueData(20000, 'games/allValid.txt')"
```

这将处理20000个棋谱，生成valueData.pt文件（可能需要一些时间）。

### 步骤2: 训练三个网络

根据README，按顺序训练三个网络：

#### 2.1 训练策略网络（如果未完成或需要重新训练）
```bash
python train.py policyNet
```
- 训练40个epoch
- 输出: policyNet.pt

#### 2.2 训练快速策略网络（如果未完成或需要重新训练）
```bash
python train.py playoutNet
```
- 训练5个epoch  
- 输出: playoutNet.pt

#### 2.3 训练价值网络
```bash
python train.py valueNet
```
- 训练8个epoch
- 输出: valueNet.pt
- 需要valueData.pt文件

## 快速训练方式

也可以使用我创建的训练脚本：

```bash
python train_all.py
```

这个脚本会自动检查所有依赖和文件，然后按顺序执行训练。

## 训练时间预估

- 准备valueData: 约10-30分钟（取决于CPU速度）
- 策略网络训练（40 epochs）: 数小时（取决于GPU）
- 快速策略网络（5 epochs）: 约30-60分钟
- 价值网络训练（8 epochs）: 约1-2小时

## 注意事项

1. **GPU加速**: 如果有CUDA GPU，训练速度会显著提升
2. **内存**: 确保有足够的内存（至少8GB推荐）
3. **磁盘空间**: 训练数据文件较大（policyData.pt约2.3GB）
4. **中断恢复**: 训练过程中如果中断，可以从断点继续（需要修改代码支持checkpoint）



